{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd25584c-c249-4427-bb38-ce4859f6000e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import (\n",
    "classification_report, accuracy_score, precision_score, recall_score, f1_score, precision_recall_fscore_support\n",
    ")\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9d32e40-ced0-4c78-b82d-9472f4086104",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agency_acronym</th>\n",
       "      <th>borough</th>\n",
       "      <th>complaint_type</th>\n",
       "      <th>location_type</th>\n",
       "      <th>time_of_day</th>\n",
       "      <th>incident_zip</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NYPD</td>\n",
       "      <td>MANHATTAN</td>\n",
       "      <td>Noise Complaints</td>\n",
       "      <td>Commercial</td>\n",
       "      <td>Evening</td>\n",
       "      <td>10031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NYPD</td>\n",
       "      <td>MANHATTAN</td>\n",
       "      <td>Noise Complaints</td>\n",
       "      <td>Residential</td>\n",
       "      <td>Evening</td>\n",
       "      <td>10013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NYPD</td>\n",
       "      <td>MANHATTAN</td>\n",
       "      <td>Noise Complaints</td>\n",
       "      <td>Commercial</td>\n",
       "      <td>Evening</td>\n",
       "      <td>10031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DPR</td>\n",
       "      <td>STATEN ISLAND</td>\n",
       "      <td>Animal-Related Complaints</td>\n",
       "      <td>Outdoor</td>\n",
       "      <td>Evening</td>\n",
       "      <td>10314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NYPD</td>\n",
       "      <td>MANHATTAN</td>\n",
       "      <td>Noise Complaints</td>\n",
       "      <td>Residential</td>\n",
       "      <td>Evening</td>\n",
       "      <td>10013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  agency_acronym        borough             complaint_type location_type  \\\n",
       "0           NYPD      MANHATTAN           Noise Complaints    Commercial   \n",
       "1           NYPD      MANHATTAN           Noise Complaints   Residential   \n",
       "2           NYPD      MANHATTAN           Noise Complaints    Commercial   \n",
       "3            DPR  STATEN ISLAND  Animal-Related Complaints       Outdoor   \n",
       "4           NYPD      MANHATTAN           Noise Complaints   Residential   \n",
       "\n",
       "  time_of_day  incident_zip  \n",
       "0     Evening         10031  \n",
       "1     Evening         10013  \n",
       "2     Evening         10031  \n",
       "3     Evening         10314  \n",
       "4     Evening         10013  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/cleaned/311_Data_Cleaned_Modeling.csv.bz2', compression = 'bz2')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f38461-23f3-41f7-be40-775714e96bbc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da9eb2c-6bde-4cc9-9337-f5880052d953",
   "metadata": {},
   "source": [
    "The first step is to split the data into X and y features and then to perform a train-test split. Although it should not matter for our dataset, generally it is best to perform a train-test split before doing encoding and scaling so that there is no data leakage. For example, if using StandardScaler on the full dataset, the StandardScaler will learn mean and standard deviation from testing data which is poor practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ec841fa-5577-471e-aecb-39b8c3a2c913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target\n",
    "X = df.drop('agency_acronym', axis = 1)\n",
    "y = df['agency_acronym']\n",
    "\n",
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0c73e0-c313-49cf-b3e4-ddec1a4cea9c",
   "metadata": {},
   "source": [
    "We need to convert all of the columns in X into one-hot encoded columns using OneHotEncoder, as they are all categorical. If there were any numerical columns, it would be best to scale them as well using StandardScaler, but that is not necessary in this case. We do not drop any of the columns when one-hot-encoding, because even though this introduces multicollinearity, we will handle this multicollinearity via regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d06aa920-1a42-4e00-8290-3093ece6863b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because all of the columns in X are categorical, we use all of the columns for OneHotEncoding\n",
    "categorical_columns = X.columns.tolist()\n",
    "\n",
    "# Create the ColumnTransformer with OneHotEncoder\n",
    "# Not dropping first as multicollinearity will be handled by regularization\n",
    "ohe = ColumnTransformer(\n",
    "    transformers = [\n",
    "        ('onehot', OneHotEncoder(), categorical_columns)\n",
    "    ],\n",
    "    remainder = 'drop'\n",
    ")\n",
    "\n",
    "# Fit the preprocessor to the training data and transform the training and testing data\n",
    "# Sparse matrices to speed up calculations and save memory\n",
    "X_train = ohe.fit_transform(X_train)\n",
    "X_test = ohe.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3626b1-c7a7-4828-8072-1f9468319ad1",
   "metadata": {},
   "source": [
    "Next, we need to convert the target variable into numerical outputs using LabelEncoder so that the values are from 0 to n_classes - 1 rather than as strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b8f1ad1-5d81-4dd7-962a-f107537b0394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate encoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform y series\n",
    "y_train = label_encoder.fit_transform(y_train)\n",
    "y_test = label_encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0023c7cd-da3d-4269-9c05-4c5f65a3a550",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2426faa8-3b73-4bc4-b3e8-447255ff6ed5",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916aa94a-fe7b-4cf0-9d8d-4bc77023247d",
   "metadata": {},
   "source": [
    "Before training, it is important to consider what an appropriate scoring metric is for using the GridSearch scoring.\n",
    "\n",
    "In the context of this project's task, Precision measures the ability of the model to correctly direct the caller to the correct agency. A low Precision means that callers are often erroneously directed to that agency. While it is possible that the caller could be redirected to the appropriate agency afterwards, that scenario involves wasted time and manpower spent redirecting the call which is not ideal when dealing with agencies such as the NYPD. Thus, Precision is a valuable metric.\n",
    "\n",
    "Recall measures the ability of the model to correctly capture all of the callers that should be directed to the correct agency. A low Recall means that the callers that should be directed to a given agency are not being directed there. This leads to lower resolution rates, longer times to resolve problems, and a general lack of effectiveness in the 311 system. Therefore, Recall is a valuable metric.\n",
    "\n",
    "Since both Precision and Recall are important, the score we will maximize is the Macro F1-Score. Macro F1-Score is used over Weighted because it treats each class as equally important, which is necessary because there are large class imbalances and it would not be helpful for the more prevalent classes to dominate the less prevalent ones. Because there are class imbalances, Accuracy is not the most reliable metric and will not be used as the primary scorer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61bacc3-883a-483b-a922-1d2e4cca23ed",
   "metadata": {},
   "source": [
    "We will use 3-fold cross-validation in the GridSearchCV to perform hyperparameter tuning. The number of folds needs to be low because the size of the data is large enough that training takes a long time. The entire training process will be timed to get a sense of computational complexity and along with the fit time for the best found model for each grid search they will be put into a dictionary. The best model will also be captured and added to the dictionary before finally being pickled to allow for loading at a later time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db32efbe-11c5-40f9-b84e-92657da6ec31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to train models\n",
    "def run_gridsearch_and_save(classifier, param_grid, X_train, y_train, pickle_filename):\n",
    "    # Record start time for the overall fitting process\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Setup GridSearchCV with f1_macro as the scoring metric\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator = classifier,\n",
    "        param_grid = param_grid,\n",
    "        cv = 3, # Reduced number of cross-validation folds to save time due to dataset size\n",
    "        scoring = 'f1_macro',\n",
    "        n_jobs = -1,\n",
    "        verbose = 1\n",
    "    )\n",
    "    \n",
    "    # Fit the model\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Record total fit time\n",
    "    total_fit_time = time.time() - start_time\n",
    "\n",
    "    # Extract relevant information\n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_params = grid_search.best_params_\n",
    "    best_model_fit_time = grid_search.cv_results_['mean_fit_time'][grid_search.best_index_]\n",
    "    num_models_tested = len(grid_search.cv_results_['params'])\n",
    "\n",
    "    # Create a dictionary to store the results\n",
    "    result_dict = {\n",
    "        'model_name': classifier.__class__.__name__,\n",
    "        'best_model': best_model,\n",
    "        'total_fit_time': total_fit_time,\n",
    "        'num_models_tested': num_models_tested,\n",
    "        'best_model_fit_time': best_model_fit_time\n",
    "    }\n",
    "\n",
    "    # Save the result dictionary to a pickle file\n",
    "    with open(pickle_filename, 'wb') as f:\n",
    "        pickle.dump(result_dict, f)\n",
    "\n",
    "    print(f\"Grid search completed. Results saved to {pickle_filename}\")\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fec38f1c-42a4-46d2-93c1-e7f21fcdf967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to load pickled models\n",
    "def load_pickle(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878172fb-c2a3-45da-bbc0-f1f48ace662f",
   "metadata": {},
   "source": [
    "#### Model Fitting / Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d3e483-1818-44d6-bac1-033801f5d0d8",
   "metadata": {},
   "source": [
    "The four model types tested are Logistic Regression via the SGDClassifier (in order to implement Stochastic Gradient Descent), a DecisionTreeClassifier and RandomForestClassifier (non-parametric models that cannot use Gradient Descent), and finally an XGBClassifier to test a boosted model that has its own learning rate. The parameter grids are deliberately small to cut down on training time. It is not ideal, but the size of the data and power of the machines available to us restricts our ability to do a more exhaustive grid search. Details of the parameters searched are noted in each cell. Each of the models were trained in Google Colab using a high memory instance (hence no print statements in this notebook)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04538328-c46b-4185-bcf7-627ae9fd1f55",
   "metadata": {},
   "source": [
    "Logistic regression is tested with various levels of regularization strength and testing different balances of L1 (Lasso) vs. L2 (Ridge) regularization using ElasticNet. To take advantage of the SGDClassifier, different learning rates are testing using the adaptive strategy which keeps the learning rate the same until no changes happen then divides the learning rate by 5 and continues.\n",
    "\n",
    "The best model used a starting learning rate of 0.1, an L1 ratio of 0.85 and the default regularization strength of 0.0001. This parameter grid fit 27 different models using 3-fold validation for a total of 81 models and had the second worst model fit time of \\~387 seconds (\\~6.5 minutes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b7c7f8b-aad4-4c06-b405-df0c10ab7132",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'SGDClassifier',\n",
       " 'best_model': SGDClassifier(eta0=0.1, l1_ratio=0.85, learning_rate='adaptive',\n",
       "               loss='log_loss', penalty='elasticnet'),\n",
       " 'total_fit_time': 3945.7306056022644,\n",
       " 'num_models_tested': 27,\n",
       " 'best_model_fit_time': 387.56030400594074}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LogisticRegression via SGDClassifier\n",
    "param_grid = {\n",
    "    'loss': ['log_loss'],               # Log loss makes SGDClassifier equivalent to LogisticRegression\n",
    "    'penalty': ['elasticnet'],          # Use the ElasticNet regularization penalty\n",
    "    'alpha': [1e-4, 1e-3, 1e-2],        # Regularization strength\n",
    "    'l1_ratio': [0.15, 0.5, 0.85],      # Percentage of the regularization weighted towards L1 vs. L2\n",
    "    'learning_rate': ['adaptive'],      # Setting the learning rate to adaptive to adjust learning rate\n",
    "    'eta0': [0.001, 0.01, 0.1],         # Initial learning rate\n",
    "    'max_iter': [1000],                 # Maximum iterations for each model\n",
    "    'tol': [1e-3],                      # Stopping criterion\n",
    "}\n",
    "\n",
    "# run_gridsearch_and_save(SGDClassifier(), param_grid, X_train, y_train, '../models/logistic_regression.pkl')\n",
    "\n",
    "logistic_regression = load_pickle('../models/logistic_regression.pkl')\n",
    "logistic_regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2cd1b2-66e6-4376-85df-452863e60c54",
   "metadata": {},
   "source": [
    "The Decision Tree is tested with a few different options for maximum depth and minimum samples split. Decision Trees are prone to overfitting so it is often helpful to reduce the maximum depth or increase the minimum samples required to create splits. Increasing the minimum samples per leaf helps prevent the tree from making sparse branches that separate very small amounts of data. Weighting based on class balances was tested to see if it was helpful due to class imbalances.\n",
    "\n",
    "The best model had the default values for max depth (None) and minimum samples split (2), likely due to the extensive preprocessing done on the data which helped to prevent extreme overfitting. The minimum samples leaf being slightly increased to 4 (from the default of 1) was beneficial. Weighting for class balances was not beneficial. 24 different models were fit using 3-fold validation for a total of 72 models. The parameter grid could possibly have been made larger, but it was kept fairly small for time constraints. The decision tree had the second fastest fit time of 195 seconds (3.25 minutes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0aff7a88-e887-4442-95e3-775ae3cc1efe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'DecisionTreeClassifier',\n",
       " 'best_model': DecisionTreeClassifier(min_samples_leaf=4, random_state=42),\n",
       " 'total_fit_time': 1043.8672311306,\n",
       " 'num_models_tested': 24,\n",
       " 'best_model_fit_time': 195.22301697731018}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DecisionTreeClassifier\n",
    "param_grid = {\n",
    "    'max_depth': [10, 20, None],        # Maximum depth of tree to reduce complexity and avoid overfitting\n",
    "    'min_samples_split': [2, 10],       # Minimum samples to split to reduce complexity and avoid overfitting\n",
    "    'min_samples_leaf': [1, 4],         # Minimum samples per leaf to reduce complexity\n",
    "    'class_weight': [None, 'balanced']  # Weights for each class to address class imbalances\n",
    "}\n",
    "# run_gridsearch_and_save(DecisionTreeClassifier(random_state = 42), param_grid,\n",
    "                        # X_train, y_train, '../models/decision_tree.pkl')\n",
    "\n",
    "decision_tree = load_pickle('../models/decision_tree.pkl')\n",
    "decision_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5d9beb-c47f-4e0f-a3f3-a21ecc9c8dad",
   "metadata": {},
   "source": [
    "The Random Forest was tested with similar parameters to the Decision Tree. The max depth for each tree does not need to be the as large as the Decision Trees, as the Random Forest is meant to use an ensemble of weaker learners. Random Forest is notably much slower to train, so the parameter grid is deliberately very small to help reduce training time. Random Forests tend to perform well without significant hyperparameter tuning.\n",
    "\n",
    "The best model used a max depth of 10 with 200 estimators and the default minimum samples split of 2 (identical to the performance of the Decision Tree). The model likely could have benefitted from a larger number of estimators due to the sheer size of the dataset but training times made that impractical. 8 models were fit using 3-fold cross validation for a total of 24 models. The Random Forest had the worst computational complexity performance with a fit time of 673 seconds (~11 minutes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "293217c2-0a53-4841-ba7a-c6cd2ce7d2f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'RandomForestClassifier',\n",
       " 'best_model': RandomForestClassifier(max_depth=10, n_estimators=200, random_state=42),\n",
       " 'total_fit_time': 1856.613802909851,\n",
       " 'num_models_tested': 8,\n",
       " 'best_model_fit_time': 673.4580012162527}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RandomForest Classifier\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],          # Number of trees to use for boosting\n",
    "    'max_depth': [5, 10],                # Maximum depth of each tree to reduce complexity and avoid overfitting\n",
    "    'min_samples_split': [2, 10],        # Minimum samples to split to reduce complexity and avoid overfitting\n",
    "}\n",
    "# run_gridsearch_and_save(RandomForestClassifier(random_state = 42), param_grid,\n",
    "#                         X_train, y_train, '../models/random_forest.pkl')\n",
    "\n",
    "random_forest = load_pickle('../models/random_forest.pkl')\n",
    "random_forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb7732f-0bb7-4bba-949b-75326a1ba9bf",
   "metadata": {},
   "source": [
    "Note that XGBoost has a different setup than the typical sklearn models. The function for fitting and training the models was not customized to adjust for the XGBoost architecture, which means that the pickled models must be trained and loaded on the same version of XGBoost. The version of XGBoost used was version 2.1.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2f0102e-e3a4-4356-aa66-ab514f40f3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install xgboost==2.1.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bf88a0-6e73-40a4-bcf9-6634b01d4b25",
   "metadata": {},
   "source": [
    "XGBoost is a boosted model, so it benefits from various learning rates and a higher number of boosting rounds. The max depth can be much lower than normal decision trees or random forest since each tree learns from the prior trees. Subsample helps to control the fraction of samples used for each tree which can help prevent overfitting by not letting each tree fit on the entire training data. XGBoost also utilizes L1 and L2 regularizations.\n",
    "\n",
    "The best model utilized a max depth of 6 with 0.01 L1 regularization and 1 L2 regularization. This has minimal regularization, but the risk of overfitting is reduced by the low max depth and by using a subsample fraction of 0.8. The best model used the larger learning rate of 0.1. It would likely be beneficial to use more estimators to have more boosting rounds, although it is possible to have too many boosting rounds and overfit the data.\n",
    "\n",
    "Notably, XGBoost is able to utilize GPU resources for faster training. The XGBoost model was fit on the A100 GPU on Google Colab. 64 models were fit with 3-fold cross validation for a total of 192 models. The XGBoost fit 192 models in less time than the Random Forest fit 24 models. XGBoost had the best computational performance with a fit time of only 167 seconds  (~2.75 minutes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3f68354-29a9-42e6-908c-5fb9b1ca9688",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'XGBClassifier',\n",
       " 'best_model': XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "               colsample_bylevel=None, colsample_bynode=None,\n",
       "               colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "               enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "               gamma=None, grow_policy=None, importance_type=None,\n",
       "               interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
       "               max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "               max_delta_step=None, max_depth=6, max_leaves=None,\n",
       "               min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "               multi_strategy=None, n_estimators=200, n_jobs=None,\n",
       "               num_parallel_tree=None, objective='multi:softprob', ...),\n",
       " 'total_fit_time': 1687.4453814029694,\n",
       " 'num_models_tested': 64,\n",
       " 'best_model_fit_time': 167.11927111943564}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# XGBoostClassifier\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],          # Number of trees to use for boosting (number of boosting rounds)\n",
    "    'learning_rate': [0.05, 0.1],        # Learning rate: step size for each boosting round\n",
    "    'max_depth': [3, 6],                 # Maximum depth of trees\n",
    "    'subsample': [0.8, 1.0],             # Fraction of samples used for each tree\n",
    "    'reg_alpha': [0.01, 0.1],            # L1 regularization term on weights\n",
    "    'reg_lambda': [1, 1.5]               # L2 regularization term on weights\n",
    "}\n",
    "# run_gridsearch_and_save(XGBClassifier(random_state = 42), param_grid, X_train, y_train, '../models/xgboost.pkl')\n",
    "\n",
    "xgboost = load_pickle('../models/xgboost.pkl')\n",
    "# The XGBoost model was trained on a GPU\n",
    "# The tree method needs to be readjusted when loading on a model without GPU support\n",
    "xgboost['best_model'].tree_method = 'auto'\n",
    "xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11b0fda9-5f74-4155-a8ef-5b21602d9ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [logistic_regression, decision_tree, random_forest, xgboost]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4604316c-0379-43c8-a11f-8beff9f7a628",
   "metadata": {},
   "source": [
    "### Scoring, Evaluation, and Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be90e1da-5070-4aef-bd84-5579f558df21",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ce4231-60d7-4d29-93a3-f41f81622b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to plot learning curves\n",
    "def plot_learning_curves(models, X, y, cv = 3,\n",
    "                         train_sizes = [0.1, 0.3, 0.5, 0.8, 1],\n",
    "                         random_state = 42, save_path = None):\n",
    "    \n",
    "    n_models = len(models)\n",
    "    \n",
    "    # Create subplots: one column per model\n",
    "    fig, axes = plt.subplots(1, n_models, figsize = (6 * n_models, 5), sharey = True)\n",
    "    \n",
    "    # If only one model, make axes a list\n",
    "    if n_models == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    # Define a color palette\n",
    "    colors = plt.cm.tab10.colors  # 10 distinct colors\n",
    "    \n",
    "    # Define the macro F1 scorer\n",
    "    scorer = make_scorer(f1_score, average = 'macro')\n",
    "    \n",
    "    for idx, (model_info, ax) in enumerate(zip(models, axes)):\n",
    "        \n",
    "        # Calculate learning curves\n",
    "        train_sizes_abs, train_scores, val_scores = learning_curve(\n",
    "            model_info[\"best_model\"], X, y,\n",
    "            cv = cv,\n",
    "            train_sizes = train_sizes,\n",
    "            scoring = scorer,\n",
    "            random_state = random_state,\n",
    "            n_jobs = -1\n",
    "        )\n",
    "        \n",
    "        # Compute mean scores across folds\n",
    "        train_scores_mean = np.mean(train_scores, axis = 1)\n",
    "        val_scores_mean = np.mean(val_scores, axis = 1)\n",
    "        \n",
    "        # Convert training sizes to percentages\n",
    "        train_sizes_pct = 100 * train_sizes_abs / X.shape[0]\n",
    "        \n",
    "        # Plot validation and training scores\n",
    "        color = colors[idx % len(colors)]\n",
    "        ax.plot(train_sizes_pct, val_scores_mean, label = \"Validation\", color = color, linestyle = '-')\n",
    "        ax.plot(train_sizes_pct, train_scores_mean, label = \"Training\", color = color, linestyle = '--')\n",
    "        \n",
    "        # Format the axis using the helper\n",
    "        ylabel = \"Macro F1-score\" if idx == 0 else \"\"  # Only first plot shows y-label\n",
    "        format_axis(ax, model_info['model_name'], \"Training Set Size (%)\", ylabel)\n",
    "        \n",
    "        ax.grid(True)\n",
    "        ax.legend()\n",
    "    \n",
    "    # Set a main title\n",
    "    fig.suptitle(\"Learning Curves (Macro F1-score)\", fontsize = 24)\n",
    "    fig.tight_layout(rect = [0, 0, 1, 0.93])\n",
    "    \n",
    "    # Save the figure if requested\n",
    "    if save_path is not None:\n",
    "        with open(save_path, 'wb') as f:\n",
    "            pickle.dump(fig, f)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5991b95b-186d-4337-ac7b-60b1ab6afd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to generate baseline model (random guessing based on class distribution)\n",
    "def generate_random_baseline(y):\n",
    "    # Get class probabilities\n",
    "    classes = np.unique(y)\n",
    "    class_counts = np.bincount(y)\n",
    "    class_probabilities = class_counts / len(y)\n",
    "    \n",
    "    # Generate random predictions based on class distribution\n",
    "    random_predictions = np.random.choice(classes, size = len(y), p = class_probabilities)\n",
    "\n",
    "    # Calculate metrics for the random baseline\n",
    "    accuracy = accuracy_score(y, random_predictions)\n",
    "    macro_precision = precision_score(y, random_predictions, average = 'macro', zero_division = 0)\n",
    "    macro_recall = recall_score(y, random_predictions, average = 'macro', zero_division = 0)\n",
    "    macro_f1 = f1_score(y, random_predictions, average = 'macro', zero_division = 0)\n",
    "\n",
    "    return accuracy, macro_precision, macro_recall, macro_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8df1211e-07df-4520-9424-e62adfd5e3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for generating a condensed classification report table as a Pandas Styled object\n",
    "def generate_classification_report_table(models_list, X_test, y_test):\n",
    "    records = []\n",
    "    \n",
    "    # Get metrics for baseline model (using y_test) that predicts according to class probabilities\n",
    "    baseline_metrics = generate_random_baseline(y_test)\n",
    "    \n",
    "    # Append baseline model to records\n",
    "    records.append({\n",
    "        'Model Name': 'Random Baseline',\n",
    "        'Accuracy': round(baseline_metrics[0], 3),\n",
    "        'Macro Precision': round(baseline_metrics[1], 3),\n",
    "        'Macro Recall': round(baseline_metrics[2], 3),\n",
    "        'Macro F1 Score': round(baseline_metrics[3], 3),\n",
    "        'Models Tested': 0,  # No models tested for baseline\n",
    "        'Best Fit Time (min)': 0  # No fit time for baseline\n",
    "    })\n",
    "\n",
    "    # For the rest of the models\n",
    "    for model_info in models_list:\n",
    "        # Create predictions\n",
    "        y_pred = model_info['best_model'].predict(X_test)\n",
    "\n",
    "        # Calculate metrics, using macro averages and accounting for possible zero division\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        macro_precision = precision_score(y_test, y_pred, average = 'macro', zero_division = 0)\n",
    "        macro_recall = recall_score(y_test, y_pred, average = 'macro', zero_division = 0)\n",
    "        macro_f1 = f1_score(y_test, y_pred, average = 'macro', zero_division = 0)\n",
    "\n",
    "        # Convert fit time to minutes\n",
    "        best_fit_time_minutes = model_info['best_model_fit_time'] / 60\n",
    "        \n",
    "        # Append record, rounding all float values to 3 decimals\n",
    "        records.append({\n",
    "            'Model Name': model_info['model_name'],\n",
    "            'Accuracy': accuracy,\n",
    "            'Macro Precision': macro_precision,\n",
    "            'Macro Recall': macro_recall,\n",
    "            'Macro F1 Score': macro_f1,\n",
    "            'Models Tested': model_info['num_models_tested'],\n",
    "            'Best Fit Time (min)': best_fit_time_minutes\n",
    "        })\n",
    "\n",
    "    # Create DataFrame to hold records\n",
    "    report_df = pd.DataFrame(records)\n",
    "\n",
    "    # Style DataFrame\n",
    "    styled_report = (\n",
    "        report_df.style\n",
    "        .set_caption(\n",
    "            \"<b style='font-size:16px; color:white'>Overall Classification Report</b>\"\n",
    "        ) # Bold larger table title\n",
    "        .format(precision = 4)  # Ensure 4 decimal places\n",
    "        .highlight_max(subset = ['Accuracy', 'Macro Precision', 'Macro Recall', 'Macro F1 Score'], \n",
    "                       color = '#355C7D', axis = 0)  # Highlight best scores\n",
    "        .set_table_styles([\n",
    "            {\n",
    "                'selector': 'th.col_heading', 'props':\n",
    "                [('border-bottom', '2px solid white'), ('text-align', 'center'), ('color', 'white')]\n",
    "            }, # White border underneath column headers, center and color white\n",
    "            {\n",
    "                'selector': 'caption', 'props': [('caption-side', 'top'), ('text-align', 'center')]\n",
    "            }, # Center align table title\n",
    "            {\n",
    "                'selector': 'td:nth-child(1)',\n",
    "                'props': [('border-right', '2px solid white')]\n",
    "            },  # White border on the right of Model Name column\n",
    "        ])\n",
    "        .set_properties(**{\n",
    "            'text-align': 'center',\n",
    "            'color': 'white'\n",
    "        }) # Center text and color white\n",
    "        .hide(axis = 'index') # Hide index\n",
    "    )\n",
    "    \n",
    "    return styled_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e03f5fa-875b-4933-8a74-21c3755c2a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for evaluating training versus testing metrics\n",
    "def training_vs_testing_metrics(models_list, X_train, y_train, X_test, y_test):\n",
    "    records = []\n",
    "\n",
    "    # Get metrics for random baseline on training and testing data\n",
    "    baseline_train_metrics = generate_random_baseline(y_train)\n",
    "    baseline_test_metrics = generate_random_baseline(y_test)\n",
    "\n",
    "    # Append random baseline metrics\n",
    "    records.append({\n",
    "        'Model Name': 'Random Baseline',\n",
    "        'Train Accuracy': baseline_train_metrics[0],\n",
    "        'Test Accuracy': baseline_test_metrics[0],\n",
    "        'Train Precision': baseline_train_metrics[1],\n",
    "        'Test Precision': baseline_test_metrics[1],\n",
    "        'Train Recall': baseline_train_metrics[2],\n",
    "        'Test Recall': baseline_test_metrics[2],\n",
    "        'Train F1 Score': baseline_train_metrics[3],\n",
    "        'Test F1 Score': baseline_test_metrics[3]\n",
    "    })\n",
    "\n",
    "    # Get metrics for the rest of the models\n",
    "    for model_info in models_list:\n",
    "        # Predict on training and testing data\n",
    "        y_train_pred = model_info['best_model'].predict(X_train)\n",
    "        y_test_pred = model_info['best_model'].predict(X_test)\n",
    "\n",
    "        # Calculate metrics for training and testing data\n",
    "        train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "        test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "        train_precision = precision_score(y_train, y_train_pred, average = 'macro', zero_division = 0)\n",
    "        test_precision = precision_score(y_test, y_test_pred, average = 'macro', zero_division = 0)\n",
    "\n",
    "        train_recall = recall_score(y_train, y_train_pred, average = 'macro', zero_division = 0)\n",
    "        test_recall = recall_score(y_test, y_test_pred, average = 'macro', zero_division = 0)\n",
    "\n",
    "        train_f1 = f1_score(y_train, y_train_pred, average = 'macro', zero_division = 0)\n",
    "        test_f1 = f1_score(y_test, y_test_pred, average = 'macro', zero_division = 0)\n",
    "\n",
    "        # Append the record, rounding all values to 4 decimals\n",
    "        records.append({\n",
    "            'Model Name': model_info['model_name'],\n",
    "            'Train Accuracy': train_accuracy,\n",
    "            'Test Accuracy': test_accuracy,\n",
    "            'Train Precision': train_precision,\n",
    "            'Test Precision': test_precision,\n",
    "            'Train Recall': train_recall,\n",
    "            'Test Recall': test_recall,\n",
    "            'Train F1 Score': train_f1,\n",
    "            'Test F1 Score': test_f1\n",
    "        })\n",
    "\n",
    "    # Create DataFrame\n",
    "    metrics_df = pd.DataFrame(records)\n",
    "\n",
    "    # Reorder columns to ensure training and testing metrics together\n",
    "    metrics_df = metrics_df[['Model Name',\n",
    "                             'Train Accuracy', 'Test Accuracy',\n",
    "                             'Train Precision', 'Test Precision',\n",
    "                             'Train Recall', 'Test Recall',\n",
    "                             'Train F1 Score', 'Test F1 Score']]\n",
    "\n",
    "    # Style the DataFrame\n",
    "    styled_df = (\n",
    "        metrics_df.style\n",
    "        .set_caption(\n",
    "            \"<b style='font-size:16px; color:white'>Classification Training vs. Test Scores</b>\"\n",
    "        )  # Bold larger table title\n",
    "        .format(precision = 4) # Ensure 4 decimal places\n",
    "        .set_table_styles([\n",
    "            {\n",
    "                'selector': 'th.col_heading', 'props':\n",
    "                [('border-bottom', '2px solid white'), ('text-align', 'center'), ('color', 'white')]\n",
    "            }, # White border underneath column headers, center and color white\n",
    "            {\n",
    "                'selector': 'caption', 'props': [('caption-side', 'top'), ('text-align', 'center')]\n",
    "            }, # Center align table title\n",
    "            {\n",
    "                'selector': 'td:nth-child(1)',\n",
    "                'props': [('border-right', '2px solid white')]\n",
    "            },  # White border on the right of Model Name column\n",
    "        ])\n",
    "        .set_properties(**{\n",
    "            'text-align': 'center',\n",
    "            'color': 'white'\n",
    "        })\n",
    "        .hide(axis = \"index\")\n",
    "    )\n",
    "    \n",
    "    return styled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bcadddc5-4da0-4490-b6c5-cdd681645023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for generating per-class side-by-side comparisons for models as a Pandas Styled object\n",
    "def generate_per_class_separate_tables(models_list, X_test, y_test, label_encoder):\n",
    "    precision_records = []\n",
    "    recall_records = []\n",
    "    f1_records = []\n",
    "\n",
    "    for model_info in models_list:\n",
    "        # Extract model name\n",
    "        model_name = model_info['model_name']\n",
    "\n",
    "        # Create predictions\n",
    "        y_pred = model_info['best_model'].predict(X_test)\n",
    "\n",
    "        # Per-class metrics handling zero division\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            y_test, y_pred, average = None, zero_division = 0\n",
    "        )\n",
    "\n",
    "        # Classes based on label_encoder\n",
    "        classes = label_encoder.inverse_transform(sorted(set(y_test)))\n",
    "        \n",
    "        for idx, cls in enumerate(classes):\n",
    "            precision_records.append({'Class': cls, model_name: precision[idx]})\n",
    "            recall_records.append({'Class': cls, model_name: recall[idx]})\n",
    "            f1_records.append({'Class': cls, model_name: f1[idx]})\n",
    "\n",
    "    # Convert to DataFrames\n",
    "    precision_df = pd.DataFrame(precision_records).groupby('Class').first()\n",
    "    recall_df = pd.DataFrame(recall_records).groupby('Class').first()\n",
    "    f1_df = pd.DataFrame(f1_records).groupby('Class').first()\n",
    "\n",
    "    # Styling function for DataFrames\n",
    "    def style_table(df, metric_name):\n",
    "        styled = (\n",
    "            df.style\n",
    "            .set_caption(\n",
    "                f\"<b style='font-size:16px; color:white'>{metric_name} per Class by Model</b>\"\n",
    "            )  # Bold larger table title\n",
    "            .format(precision = 4)  # Keep 4 decimal places\n",
    "            .highlight_max(axis = 1, color = '#355C7D')  # Highlight best scores\n",
    "            .set_table_styles([\n",
    "                {\n",
    "                    'selector': 'th', 'props':\n",
    "                    [('border-bottom', '2px solid white'), ('text-align', 'center'), ('color', 'white')]\n",
    "                }, # White border under column level, center and color white\n",
    "                {\n",
    "                    'selector': 'th.col_heading', 'props':\n",
    "                    [('border-bottom', '2px solid white'), ('text-align', 'center'), ('color', 'white')]\n",
    "                }, # Second white border under column headers, center and color white\n",
    "                {\n",
    "                    'selector': 'th.row_heading', 'props':\n",
    "                    [('border-right', '2px solid white'), ('text-align', 'center'), ('color', 'white')]\n",
    "                }, # White border to the right of index which contains class names, center and color white\n",
    "                {\n",
    "                    'selector': 'caption', 'props': [('caption-side', 'top'), ('text-align', 'center')]\n",
    "                } # Center align title\n",
    "            ])\n",
    "            .set_properties(**{\n",
    "                'text-align': 'center',\n",
    "                'color': 'white'\n",
    "            }) # Center align text and color white\n",
    "        )\n",
    "        return styled\n",
    "\n",
    "    # Style each DataFrame\n",
    "    styled_precision = style_table(precision_df, \"Precision\")\n",
    "    styled_recall = style_table(recall_df, \"Recall\")\n",
    "    styled_f1 = style_table(f1_df, \"F1 Score\")\n",
    "\n",
    "    return styled_precision, styled_recall, styled_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302bda41-789a-406d-8b65-bd0bad46b484",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0f5103-8916-4f24-b53f-6b0f36c4be49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit using Google Colab - takes a long time to calculate\n",
    "# plot_learning_curves(models, X_train, y_train, save_path = 'learning_curves.pkl')\n",
    "\n",
    "# Loading saved figure from pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c1f968-bc0e-4be8-8ed8-59d83fd4ab05",
   "metadata": {},
   "source": [
    "Viewing the overall classification scores, the Decision Tree and the XGBoost both performed very similarly. Although the Decision Tree had slightly better Macro F1, Precision and Accuracy compared to the XGBoost, the stats between the two models are nearly identical. Going strictly off of the main metric we have chosen for scoring (Macro F1), the Decision Tree is the winner, but the XGBoost model was faster to train and thus is likely more scalable for practically identical performance. Either of these two models seem appropriate.\n",
    "\n",
    "The Random Forest performed the worst overall with a particularly low Macro Recall score. It also was the worst performing model in terms of fit time, meaning it is not likely to be scalable. The Logistic Regression performed slightly worse than the Decision Tree and XGBoost, but it had a much worse fit time than those models. For both of those reasons, Logistic Regression is not preferred.\n",
    "\n",
    "All of the models significantly outperformed a baseline classifier that simply predicted randomly according to class probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a85a39fa-e996-43be-909f-746bb5e2342c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_fa129 th.col_heading {\n",
       "  border-bottom: 2px solid white;\n",
       "  text-align: center;\n",
       "  color: white;\n",
       "}\n",
       "#T_fa129 caption {\n",
       "  caption-side: top;\n",
       "  text-align: center;\n",
       "}\n",
       "#T_fa129 td:nth-child(1) {\n",
       "  border-right: 2px solid white;\n",
       "}\n",
       "#T_fa129_row0_col0, #T_fa129_row0_col1, #T_fa129_row0_col2, #T_fa129_row0_col3, #T_fa129_row0_col4, #T_fa129_row0_col5, #T_fa129_row0_col6, #T_fa129_row1_col0, #T_fa129_row1_col1, #T_fa129_row1_col2, #T_fa129_row1_col3, #T_fa129_row1_col4, #T_fa129_row1_col5, #T_fa129_row1_col6, #T_fa129_row2_col0, #T_fa129_row2_col2, #T_fa129_row2_col3, #T_fa129_row2_col5, #T_fa129_row2_col6, #T_fa129_row3_col0, #T_fa129_row3_col1, #T_fa129_row3_col3, #T_fa129_row3_col4, #T_fa129_row3_col5, #T_fa129_row3_col6, #T_fa129_row4_col0, #T_fa129_row4_col1, #T_fa129_row4_col2, #T_fa129_row4_col4, #T_fa129_row4_col5, #T_fa129_row4_col6 {\n",
       "  text-align: center;\n",
       "  color: white;\n",
       "}\n",
       "#T_fa129_row2_col1, #T_fa129_row2_col4, #T_fa129_row3_col2, #T_fa129_row4_col3 {\n",
       "  background-color: #355C7D;\n",
       "  text-align: center;\n",
       "  color: white;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_fa129\">\n",
       "  <caption><b style='font-size:16px; color:white'>Overall Classification Report</b></caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_fa129_level0_col0\" class=\"col_heading level0 col0\" >Model Name</th>\n",
       "      <th id=\"T_fa129_level0_col1\" class=\"col_heading level0 col1\" >Accuracy</th>\n",
       "      <th id=\"T_fa129_level0_col2\" class=\"col_heading level0 col2\" >Macro Precision</th>\n",
       "      <th id=\"T_fa129_level0_col3\" class=\"col_heading level0 col3\" >Macro Recall</th>\n",
       "      <th id=\"T_fa129_level0_col4\" class=\"col_heading level0 col4\" >Macro F1 Score</th>\n",
       "      <th id=\"T_fa129_level0_col5\" class=\"col_heading level0 col5\" >Models Tested</th>\n",
       "      <th id=\"T_fa129_level0_col6\" class=\"col_heading level0 col6\" >Best Fit Time (min)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_fa129_row0_col0\" class=\"data row0 col0\" >Random Baseline</td>\n",
       "      <td id=\"T_fa129_row0_col1\" class=\"data row0 col1\" >0.2730</td>\n",
       "      <td id=\"T_fa129_row0_col2\" class=\"data row0 col2\" >0.1000</td>\n",
       "      <td id=\"T_fa129_row0_col3\" class=\"data row0 col3\" >0.1000</td>\n",
       "      <td id=\"T_fa129_row0_col4\" class=\"data row0 col4\" >0.1000</td>\n",
       "      <td id=\"T_fa129_row0_col5\" class=\"data row0 col5\" >0</td>\n",
       "      <td id=\"T_fa129_row0_col6\" class=\"data row0 col6\" >0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_fa129_row1_col0\" class=\"data row1 col0\" >SGDClassifier</td>\n",
       "      <td id=\"T_fa129_row1_col1\" class=\"data row1 col1\" >0.8958</td>\n",
       "      <td id=\"T_fa129_row1_col2\" class=\"data row1 col2\" >0.8501</td>\n",
       "      <td id=\"T_fa129_row1_col3\" class=\"data row1 col3\" >0.7921</td>\n",
       "      <td id=\"T_fa129_row1_col4\" class=\"data row1 col4\" >0.8082</td>\n",
       "      <td id=\"T_fa129_row1_col5\" class=\"data row1 col5\" >27</td>\n",
       "      <td id=\"T_fa129_row1_col6\" class=\"data row1 col6\" >6.4593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_fa129_row2_col0\" class=\"data row2 col0\" >DecisionTreeClassifier</td>\n",
       "      <td id=\"T_fa129_row2_col1\" class=\"data row2 col1\" >0.9035</td>\n",
       "      <td id=\"T_fa129_row2_col2\" class=\"data row2 col2\" >0.8638</td>\n",
       "      <td id=\"T_fa129_row2_col3\" class=\"data row2 col3\" >0.8051</td>\n",
       "      <td id=\"T_fa129_row2_col4\" class=\"data row2 col4\" >0.8248</td>\n",
       "      <td id=\"T_fa129_row2_col5\" class=\"data row2 col5\" >24</td>\n",
       "      <td id=\"T_fa129_row2_col6\" class=\"data row2 col6\" >3.2537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_fa129_row3_col0\" class=\"data row3 col0\" >RandomForestClassifier</td>\n",
       "      <td id=\"T_fa129_row3_col1\" class=\"data row3 col1\" >0.8640</td>\n",
       "      <td id=\"T_fa129_row3_col2\" class=\"data row3 col2\" >0.8695</td>\n",
       "      <td id=\"T_fa129_row3_col3\" class=\"data row3 col3\" >0.6945</td>\n",
       "      <td id=\"T_fa129_row3_col4\" class=\"data row3 col4\" >0.7431</td>\n",
       "      <td id=\"T_fa129_row3_col5\" class=\"data row3 col5\" >8</td>\n",
       "      <td id=\"T_fa129_row3_col6\" class=\"data row3 col6\" >11.2243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_fa129_row4_col0\" class=\"data row4 col0\" >XGBClassifier</td>\n",
       "      <td id=\"T_fa129_row4_col1\" class=\"data row4 col1\" >0.9023</td>\n",
       "      <td id=\"T_fa129_row4_col2\" class=\"data row4 col2\" >0.8622</td>\n",
       "      <td id=\"T_fa129_row4_col3\" class=\"data row4 col3\" >0.8072</td>\n",
       "      <td id=\"T_fa129_row4_col4\" class=\"data row4 col4\" >0.8233</td>\n",
       "      <td id=\"T_fa129_row4_col5\" class=\"data row4 col5\" >64</td>\n",
       "      <td id=\"T_fa129_row4_col6\" class=\"data row4 col6\" >2.7853</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x70bba4bcf680>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_classification_report_table(models, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19dfec3-b59c-489e-95a8-9a681afea845",
   "metadata": {},
   "source": [
    "Next, it is worthwhile to check for overfitting or underfitting in the models. There appears to be effectively no evidence of overfitting in the models, as the training and testing scores are nearly identical across metrics. Given the strength of the features that were crafted during the data preparation process, it would be hard to say that any of the models are underfitting except for perhaps the Random Forest.\n",
    "\n",
    "Perhaps greater feature selection or engineering would produce better results, or maybe even more complex hyperparameter tuning, but generally speaking, macro scores in the 80-85%+ range would indicate that the models are performing very well in the face of large class imbalances (NYPD makes up 46% of the data, HPD makes up 21% of the data, and the next highest class representation is the DSNY at 9% of the data). Considering the extremely poor performance of a baseline model that randomly predicts according to class probabilities, it seems safe to say that the models are not underfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "30cc18f7-78ba-4ab5-8b57-7d3a52f2ecc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_764e6 th.col_heading {\n",
       "  border-bottom: 2px solid white;\n",
       "  text-align: center;\n",
       "  color: white;\n",
       "}\n",
       "#T_764e6 caption {\n",
       "  caption-side: top;\n",
       "  text-align: center;\n",
       "}\n",
       "#T_764e6 td:nth-child(1) {\n",
       "  border-right: 2px solid white;\n",
       "}\n",
       "#T_764e6_row0_col0, #T_764e6_row0_col1, #T_764e6_row0_col2, #T_764e6_row0_col3, #T_764e6_row0_col4, #T_764e6_row0_col5, #T_764e6_row0_col6, #T_764e6_row0_col7, #T_764e6_row0_col8, #T_764e6_row1_col0, #T_764e6_row1_col1, #T_764e6_row1_col2, #T_764e6_row1_col3, #T_764e6_row1_col4, #T_764e6_row1_col5, #T_764e6_row1_col6, #T_764e6_row1_col7, #T_764e6_row1_col8, #T_764e6_row2_col0, #T_764e6_row2_col1, #T_764e6_row2_col2, #T_764e6_row2_col3, #T_764e6_row2_col4, #T_764e6_row2_col5, #T_764e6_row2_col6, #T_764e6_row2_col7, #T_764e6_row2_col8, #T_764e6_row3_col0, #T_764e6_row3_col1, #T_764e6_row3_col2, #T_764e6_row3_col3, #T_764e6_row3_col4, #T_764e6_row3_col5, #T_764e6_row3_col6, #T_764e6_row3_col7, #T_764e6_row3_col8, #T_764e6_row4_col0, #T_764e6_row4_col1, #T_764e6_row4_col2, #T_764e6_row4_col3, #T_764e6_row4_col4, #T_764e6_row4_col5, #T_764e6_row4_col6, #T_764e6_row4_col7, #T_764e6_row4_col8 {\n",
       "  text-align: center;\n",
       "  color: white;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_764e6\">\n",
       "  <caption><b style='font-size:16px; color:white'>Classification Training vs. Test Scores</b></caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_764e6_level0_col0\" class=\"col_heading level0 col0\" >Model Name</th>\n",
       "      <th id=\"T_764e6_level0_col1\" class=\"col_heading level0 col1\" >Train Accuracy</th>\n",
       "      <th id=\"T_764e6_level0_col2\" class=\"col_heading level0 col2\" >Test Accuracy</th>\n",
       "      <th id=\"T_764e6_level0_col3\" class=\"col_heading level0 col3\" >Train Precision</th>\n",
       "      <th id=\"T_764e6_level0_col4\" class=\"col_heading level0 col4\" >Test Precision</th>\n",
       "      <th id=\"T_764e6_level0_col5\" class=\"col_heading level0 col5\" >Train Recall</th>\n",
       "      <th id=\"T_764e6_level0_col6\" class=\"col_heading level0 col6\" >Test Recall</th>\n",
       "      <th id=\"T_764e6_level0_col7\" class=\"col_heading level0 col7\" >Train F1 Score</th>\n",
       "      <th id=\"T_764e6_level0_col8\" class=\"col_heading level0 col8\" >Test F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_764e6_row0_col0\" class=\"data row0 col0\" >Random Baseline</td>\n",
       "      <td id=\"T_764e6_row0_col1\" class=\"data row0 col1\" >0.2736</td>\n",
       "      <td id=\"T_764e6_row0_col2\" class=\"data row0 col2\" >0.2735</td>\n",
       "      <td id=\"T_764e6_row0_col3\" class=\"data row0 col3\" >0.1000</td>\n",
       "      <td id=\"T_764e6_row0_col4\" class=\"data row0 col4\" >0.1001</td>\n",
       "      <td id=\"T_764e6_row0_col5\" class=\"data row0 col5\" >0.1000</td>\n",
       "      <td id=\"T_764e6_row0_col6\" class=\"data row0 col6\" >0.1001</td>\n",
       "      <td id=\"T_764e6_row0_col7\" class=\"data row0 col7\" >0.1000</td>\n",
       "      <td id=\"T_764e6_row0_col8\" class=\"data row0 col8\" >0.1001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_764e6_row1_col0\" class=\"data row1 col0\" >SGDClassifier</td>\n",
       "      <td id=\"T_764e6_row1_col1\" class=\"data row1 col1\" >0.8955</td>\n",
       "      <td id=\"T_764e6_row1_col2\" class=\"data row1 col2\" >0.8958</td>\n",
       "      <td id=\"T_764e6_row1_col3\" class=\"data row1 col3\" >0.8496</td>\n",
       "      <td id=\"T_764e6_row1_col4\" class=\"data row1 col4\" >0.8501</td>\n",
       "      <td id=\"T_764e6_row1_col5\" class=\"data row1 col5\" >0.7912</td>\n",
       "      <td id=\"T_764e6_row1_col6\" class=\"data row1 col6\" >0.7921</td>\n",
       "      <td id=\"T_764e6_row1_col7\" class=\"data row1 col7\" >0.8073</td>\n",
       "      <td id=\"T_764e6_row1_col8\" class=\"data row1 col8\" >0.8082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_764e6_row2_col0\" class=\"data row2 col0\" >DecisionTreeClassifier</td>\n",
       "      <td id=\"T_764e6_row2_col1\" class=\"data row2 col1\" >0.9049</td>\n",
       "      <td id=\"T_764e6_row2_col2\" class=\"data row2 col2\" >0.9035</td>\n",
       "      <td id=\"T_764e6_row2_col3\" class=\"data row2 col3\" >0.8677</td>\n",
       "      <td id=\"T_764e6_row2_col4\" class=\"data row2 col4\" >0.8638</td>\n",
       "      <td id=\"T_764e6_row2_col5\" class=\"data row2 col5\" >0.8082</td>\n",
       "      <td id=\"T_764e6_row2_col6\" class=\"data row2 col6\" >0.8051</td>\n",
       "      <td id=\"T_764e6_row2_col7\" class=\"data row2 col7\" >0.8280</td>\n",
       "      <td id=\"T_764e6_row2_col8\" class=\"data row2 col8\" >0.8248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_764e6_row3_col0\" class=\"data row3 col0\" >RandomForestClassifier</td>\n",
       "      <td id=\"T_764e6_row3_col1\" class=\"data row3 col1\" >0.8640</td>\n",
       "      <td id=\"T_764e6_row3_col2\" class=\"data row3 col2\" >0.8640</td>\n",
       "      <td id=\"T_764e6_row3_col3\" class=\"data row3 col3\" >0.8704</td>\n",
       "      <td id=\"T_764e6_row3_col4\" class=\"data row3 col4\" >0.8695</td>\n",
       "      <td id=\"T_764e6_row3_col5\" class=\"data row3 col5\" >0.6944</td>\n",
       "      <td id=\"T_764e6_row3_col6\" class=\"data row3 col6\" >0.6945</td>\n",
       "      <td id=\"T_764e6_row3_col7\" class=\"data row3 col7\" >0.7431</td>\n",
       "      <td id=\"T_764e6_row3_col8\" class=\"data row3 col8\" >0.7431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_764e6_row4_col0\" class=\"data row4 col0\" >XGBClassifier</td>\n",
       "      <td id=\"T_764e6_row4_col1\" class=\"data row4 col1\" >0.9024</td>\n",
       "      <td id=\"T_764e6_row4_col2\" class=\"data row4 col2\" >0.9023</td>\n",
       "      <td id=\"T_764e6_row4_col3\" class=\"data row4 col3\" >0.8631</td>\n",
       "      <td id=\"T_764e6_row4_col4\" class=\"data row4 col4\" >0.8622</td>\n",
       "      <td id=\"T_764e6_row4_col5\" class=\"data row4 col5\" >0.8077</td>\n",
       "      <td id=\"T_764e6_row4_col6\" class=\"data row4 col6\" >0.8072</td>\n",
       "      <td id=\"T_764e6_row4_col7\" class=\"data row4 col7\" >0.8238</td>\n",
       "      <td id=\"T_764e6_row4_col8\" class=\"data row4 col8\" >0.8233</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x70bb87f67bc0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_vs_testing_metrics(models, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacde4e2-9fc8-403f-aadc-50619070b729",
   "metadata": {},
   "source": [
    "The next step is to identify class-by-class performance to see where each model struggled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dffd4476-bca1-483f-a606-af06b333bd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "precisions, recalls, f1s = generate_per_class_separate_tables(models, X_test, y_test, label_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a94e28-59cf-4a1f-89a2-b284291ddc36",
   "metadata": {},
   "source": [
    "The Random Forest had the best overall performance with regards to precision. Notably, the Random Forest performed particularly well on the lower frequency classes of DHS, DOHMH, and Other. However, the Random Forest also struggled with precision on the most common classes of NYPD and HPD, at least in comparison to the other classifiers, which means the Random Forest was overconfident about the high frequency classes. The XGBoost model has the highest precision on the NYPD, meaning it is the most likely to not misdirect calls to the NYPD and waste valuable police resources.\n",
    "\n",
    "All of the other classifiers performed relatively similarly across classes. The DOB and DPR were the hardest classes to predict precisely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "94fc40c1-016c-4fa3-9b45-1003ce42e36f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_28a2d th {\n",
       "  border-bottom: 2px solid white;\n",
       "  text-align: center;\n",
       "  color: white;\n",
       "}\n",
       "#T_28a2d th.col_heading {\n",
       "  border-bottom: 2px solid white;\n",
       "  text-align: center;\n",
       "  color: white;\n",
       "}\n",
       "#T_28a2d th.row_heading {\n",
       "  border-right: 2px solid white;\n",
       "  text-align: center;\n",
       "  color: white;\n",
       "}\n",
       "#T_28a2d caption {\n",
       "  caption-side: top;\n",
       "  text-align: center;\n",
       "}\n",
       "#T_28a2d_row0_col0, #T_28a2d_row0_col1, #T_28a2d_row0_col2, #T_28a2d_row1_col0, #T_28a2d_row1_col1, #T_28a2d_row1_col3, #T_28a2d_row2_col0, #T_28a2d_row2_col2, #T_28a2d_row2_col3, #T_28a2d_row3_col0, #T_28a2d_row3_col1, #T_28a2d_row3_col3, #T_28a2d_row4_col1, #T_28a2d_row4_col2, #T_28a2d_row4_col3, #T_28a2d_row5_col0, #T_28a2d_row5_col2, #T_28a2d_row5_col3, #T_28a2d_row6_col0, #T_28a2d_row6_col1, #T_28a2d_row6_col3, #T_28a2d_row7_col1, #T_28a2d_row7_col2, #T_28a2d_row7_col3, #T_28a2d_row8_col0, #T_28a2d_row8_col1, #T_28a2d_row8_col2, #T_28a2d_row9_col0, #T_28a2d_row9_col1, #T_28a2d_row9_col3 {\n",
       "  text-align: center;\n",
       "  color: white;\n",
       "}\n",
       "#T_28a2d_row0_col3, #T_28a2d_row1_col2, #T_28a2d_row2_col1, #T_28a2d_row3_col2, #T_28a2d_row4_col0, #T_28a2d_row5_col1, #T_28a2d_row6_col2, #T_28a2d_row7_col0, #T_28a2d_row8_col3, #T_28a2d_row9_col2 {\n",
       "  background-color: #355C7D;\n",
       "  text-align: center;\n",
       "  color: white;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_28a2d\">\n",
       "  <caption><b style='font-size:16px; color:white'>Precision per Class by Model</b></caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_28a2d_level0_col0\" class=\"col_heading level0 col0\" >SGDClassifier</th>\n",
       "      <th id=\"T_28a2d_level0_col1\" class=\"col_heading level0 col1\" >DecisionTreeClassifier</th>\n",
       "      <th id=\"T_28a2d_level0_col2\" class=\"col_heading level0 col2\" >RandomForestClassifier</th>\n",
       "      <th id=\"T_28a2d_level0_col3\" class=\"col_heading level0 col3\" >XGBClassifier</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Class</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_28a2d_level0_row0\" class=\"row_heading level0 row0\" >DEP</th>\n",
       "      <td id=\"T_28a2d_row0_col0\" class=\"data row0 col0\" >0.9517</td>\n",
       "      <td id=\"T_28a2d_row0_col1\" class=\"data row0 col1\" >0.9503</td>\n",
       "      <td id=\"T_28a2d_row0_col2\" class=\"data row0 col2\" >0.9405</td>\n",
       "      <td id=\"T_28a2d_row0_col3\" class=\"data row0 col3\" >0.9570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_28a2d_level0_row1\" class=\"row_heading level0 row1\" >DHS</th>\n",
       "      <td id=\"T_28a2d_row1_col0\" class=\"data row1 col0\" >0.7541</td>\n",
       "      <td id=\"T_28a2d_row1_col1\" class=\"data row1 col1\" >0.7804</td>\n",
       "      <td id=\"T_28a2d_row1_col2\" class=\"data row1 col2\" >0.8048</td>\n",
       "      <td id=\"T_28a2d_row1_col3\" class=\"data row1 col3\" >0.7678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_28a2d_level0_row2\" class=\"row_heading level0 row2\" >DOB</th>\n",
       "      <td id=\"T_28a2d_row2_col0\" class=\"data row2 col0\" >0.7501</td>\n",
       "      <td id=\"T_28a2d_row2_col1\" class=\"data row2 col1\" >0.7741</td>\n",
       "      <td id=\"T_28a2d_row2_col2\" class=\"data row2 col2\" >0.7533</td>\n",
       "      <td id=\"T_28a2d_row2_col3\" class=\"data row2 col3\" >0.7631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_28a2d_level0_row3\" class=\"row_heading level0 row3\" >DOHMH</th>\n",
       "      <td id=\"T_28a2d_row3_col0\" class=\"data row3 col0\" >0.8241</td>\n",
       "      <td id=\"T_28a2d_row3_col1\" class=\"data row3 col1\" >0.8399</td>\n",
       "      <td id=\"T_28a2d_row3_col2\" class=\"data row3 col2\" >0.8818</td>\n",
       "      <td id=\"T_28a2d_row3_col3\" class=\"data row3 col3\" >0.8398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_28a2d_level0_row4\" class=\"row_heading level0 row4\" >DOT</th>\n",
       "      <td id=\"T_28a2d_row4_col0\" class=\"data row4 col0\" >0.9698</td>\n",
       "      <td id=\"T_28a2d_row4_col1\" class=\"data row4 col1\" >0.9394</td>\n",
       "      <td id=\"T_28a2d_row4_col2\" class=\"data row4 col2\" >0.9669</td>\n",
       "      <td id=\"T_28a2d_row4_col3\" class=\"data row4 col3\" >0.9604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_28a2d_level0_row5\" class=\"row_heading level0 row5\" >DPR</th>\n",
       "      <td id=\"T_28a2d_row5_col0\" class=\"data row5 col0\" >0.7302</td>\n",
       "      <td id=\"T_28a2d_row5_col1\" class=\"data row5 col1\" >0.7586</td>\n",
       "      <td id=\"T_28a2d_row5_col2\" class=\"data row5 col2\" >0.7356</td>\n",
       "      <td id=\"T_28a2d_row5_col3\" class=\"data row5 col3\" >0.7444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_28a2d_level0_row6\" class=\"row_heading level0 row6\" >DSNY</th>\n",
       "      <td id=\"T_28a2d_row6_col0\" class=\"data row6 col0\" >0.7836</td>\n",
       "      <td id=\"T_28a2d_row6_col1\" class=\"data row6 col1\" >0.8332</td>\n",
       "      <td id=\"T_28a2d_row6_col2\" class=\"data row6 col2\" >0.9244</td>\n",
       "      <td id=\"T_28a2d_row6_col3\" class=\"data row6 col3\" >0.8101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_28a2d_level0_row7\" class=\"row_heading level0 row7\" >HPD</th>\n",
       "      <td id=\"T_28a2d_row7_col0\" class=\"data row7 col0\" >0.9804</td>\n",
       "      <td id=\"T_28a2d_row7_col1\" class=\"data row7 col1\" >0.9783</td>\n",
       "      <td id=\"T_28a2d_row7_col2\" class=\"data row7 col2\" >0.9599</td>\n",
       "      <td id=\"T_28a2d_row7_col3\" class=\"data row7 col3\" >0.9774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_28a2d_level0_row8\" class=\"row_heading level0 row8\" >NYPD</th>\n",
       "      <td id=\"T_28a2d_row8_col0\" class=\"data row8 col0\" >0.9016</td>\n",
       "      <td id=\"T_28a2d_row8_col1\" class=\"data row8 col1\" >0.9028</td>\n",
       "      <td id=\"T_28a2d_row8_col2\" class=\"data row8 col2\" >0.8254</td>\n",
       "      <td id=\"T_28a2d_row8_col3\" class=\"data row8 col3\" >0.9059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_28a2d_level0_row9\" class=\"row_heading level0 row9\" >Other</th>\n",
       "      <td id=\"T_28a2d_row9_col0\" class=\"data row9 col0\" >0.8560</td>\n",
       "      <td id=\"T_28a2d_row9_col1\" class=\"data row9 col1\" >0.8815</td>\n",
       "      <td id=\"T_28a2d_row9_col2\" class=\"data row9 col2\" >0.9022</td>\n",
       "      <td id=\"T_28a2d_row9_col3\" class=\"data row9 col3\" >0.8964</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x70bb87fbf560>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precisions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d88e851-e0b8-473a-9f97-71b95751a77f",
   "metadata": {},
   "source": [
    "The Random Forest clearly favors the majority classes, achieving nearly perfect recall on both NYPD and HPD. The XGBoost model has the best overall recall and performed quite a bit better on the DHS category than the other models. Interestingly, the Logistic Regression outperforms the Decision Tree in a number of classes (DOB, DPR, and DSNY).\n",
    "\n",
    "All of the classes, besides the Random Forest, performed fairly similarly across classes. The hardest classes to attain high recall on were DOT, Other, and DHS. Fortunately, all of the models had fairly high recall on the majority classes of the HPD and NYPD, as it is important for the most common agencies to actually receive calls that should be directed to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f3f25c92-9ad5-4774-97b6-798ce9a5a924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_d650b th {\n",
       "  border-bottom: 2px solid white;\n",
       "  text-align: center;\n",
       "  color: white;\n",
       "}\n",
       "#T_d650b th.col_heading {\n",
       "  border-bottom: 2px solid white;\n",
       "  text-align: center;\n",
       "  color: white;\n",
       "}\n",
       "#T_d650b th.row_heading {\n",
       "  border-right: 2px solid white;\n",
       "  text-align: center;\n",
       "  color: white;\n",
       "}\n",
       "#T_d650b caption {\n",
       "  caption-side: top;\n",
       "  text-align: center;\n",
       "}\n",
       "#T_d650b_row0_col0, #T_d650b_row0_col2, #T_d650b_row0_col3, #T_d650b_row1_col0, #T_d650b_row1_col1, #T_d650b_row1_col2, #T_d650b_row2_col1, #T_d650b_row2_col2, #T_d650b_row2_col3, #T_d650b_row3_col0, #T_d650b_row3_col2, #T_d650b_row3_col3, #T_d650b_row4_col0, #T_d650b_row4_col2, #T_d650b_row4_col3, #T_d650b_row5_col0, #T_d650b_row5_col1, #T_d650b_row5_col2, #T_d650b_row6_col0, #T_d650b_row6_col1, #T_d650b_row6_col2, #T_d650b_row7_col0, #T_d650b_row7_col1, #T_d650b_row7_col3, #T_d650b_row8_col0, #T_d650b_row8_col1, #T_d650b_row8_col3, #T_d650b_row9_col0, #T_d650b_row9_col2, #T_d650b_row9_col3 {\n",
       "  text-align: center;\n",
       "  color: white;\n",
       "}\n",
       "#T_d650b_row0_col1, #T_d650b_row1_col3, #T_d650b_row2_col0, #T_d650b_row3_col1, #T_d650b_row4_col1, #T_d650b_row5_col3, #T_d650b_row6_col3, #T_d650b_row7_col2, #T_d650b_row8_col2, #T_d650b_row9_col1 {\n",
       "  background-color: #355C7D;\n",
       "  text-align: center;\n",
       "  color: white;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_d650b\">\n",
       "  <caption><b style='font-size:16px; color:white'>Recall per Class by Model</b></caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_d650b_level0_col0\" class=\"col_heading level0 col0\" >SGDClassifier</th>\n",
       "      <th id=\"T_d650b_level0_col1\" class=\"col_heading level0 col1\" >DecisionTreeClassifier</th>\n",
       "      <th id=\"T_d650b_level0_col2\" class=\"col_heading level0 col2\" >RandomForestClassifier</th>\n",
       "      <th id=\"T_d650b_level0_col3\" class=\"col_heading level0 col3\" >XGBClassifier</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Class</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_d650b_level0_row0\" class=\"row_heading level0 row0\" >DEP</th>\n",
       "      <td id=\"T_d650b_row0_col0\" class=\"data row0 col0\" >0.8559</td>\n",
       "      <td id=\"T_d650b_row0_col1\" class=\"data row0 col1\" >0.8689</td>\n",
       "      <td id=\"T_d650b_row0_col2\" class=\"data row0 col2\" >0.8524</td>\n",
       "      <td id=\"T_d650b_row0_col3\" class=\"data row0 col3\" >0.8620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d650b_level0_row1\" class=\"row_heading level0 row1\" >DHS</th>\n",
       "      <td id=\"T_d650b_row1_col0\" class=\"data row1 col0\" >0.6652</td>\n",
       "      <td id=\"T_d650b_row1_col1\" class=\"data row1 col1\" >0.6863</td>\n",
       "      <td id=\"T_d650b_row1_col2\" class=\"data row1 col2\" >0.4557</td>\n",
       "      <td id=\"T_d650b_row1_col3\" class=\"data row1 col3\" >0.7002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d650b_level0_row2\" class=\"row_heading level0 row2\" >DOB</th>\n",
       "      <td id=\"T_d650b_row2_col0\" class=\"data row2 col0\" >0.9977</td>\n",
       "      <td id=\"T_d650b_row2_col1\" class=\"data row2 col1\" >0.9578</td>\n",
       "      <td id=\"T_d650b_row2_col2\" class=\"data row2 col2\" >0.9803</td>\n",
       "      <td id=\"T_d650b_row2_col3\" class=\"data row2 col3\" >0.9811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d650b_level0_row3\" class=\"row_heading level0 row3\" >DOHMH</th>\n",
       "      <td id=\"T_d650b_row3_col0\" class=\"data row3 col0\" >0.7163</td>\n",
       "      <td id=\"T_d650b_row3_col1\" class=\"data row3 col1\" >0.7818</td>\n",
       "      <td id=\"T_d650b_row3_col2\" class=\"data row3 col2\" >0.4275</td>\n",
       "      <td id=\"T_d650b_row3_col3\" class=\"data row3 col3\" >0.7735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d650b_level0_row4\" class=\"row_heading level0 row4\" >DOT</th>\n",
       "      <td id=\"T_d650b_row4_col0\" class=\"data row4 col0\" >0.5538</td>\n",
       "      <td id=\"T_d650b_row4_col1\" class=\"data row4 col1\" >0.5829</td>\n",
       "      <td id=\"T_d650b_row4_col2\" class=\"data row4 col2\" >0.5571</td>\n",
       "      <td id=\"T_d650b_row4_col3\" class=\"data row4 col3\" >0.5677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d650b_level0_row5\" class=\"row_heading level0 row5\" >DPR</th>\n",
       "      <td id=\"T_d650b_row5_col0\" class=\"data row5 col0\" >0.9140</td>\n",
       "      <td id=\"T_d650b_row5_col1\" class=\"data row5 col1\" >0.9020</td>\n",
       "      <td id=\"T_d650b_row5_col2\" class=\"data row5 col2\" >0.8252</td>\n",
       "      <td id=\"T_d650b_row5_col3\" class=\"data row5 col3\" >0.9168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d650b_level0_row6\" class=\"row_heading level0 row6\" >DSNY</th>\n",
       "      <td id=\"T_d650b_row6_col0\" class=\"data row6 col0\" >0.7205</td>\n",
       "      <td id=\"T_d650b_row6_col1\" class=\"data row6 col1\" >0.7070</td>\n",
       "      <td id=\"T_d650b_row6_col2\" class=\"data row6 col2\" >0.3678</td>\n",
       "      <td id=\"T_d650b_row6_col3\" class=\"data row6 col3\" >0.7249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d650b_level0_row7\" class=\"row_heading level0 row7\" >HPD</th>\n",
       "      <td id=\"T_d650b_row7_col0\" class=\"data row7 col0\" >0.9831</td>\n",
       "      <td id=\"T_d650b_row7_col1\" class=\"data row7 col1\" >0.9931</td>\n",
       "      <td id=\"T_d650b_row7_col2\" class=\"data row7 col2\" >0.9932</td>\n",
       "      <td id=\"T_d650b_row7_col3\" class=\"data row7 col3\" >0.9931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d650b_level0_row8\" class=\"row_heading level0 row8\" >NYPD</th>\n",
       "      <td id=\"T_d650b_row8_col0\" class=\"data row8 col0\" >0.9652</td>\n",
       "      <td id=\"T_d650b_row8_col1\" class=\"data row8 col1\" >0.9718</td>\n",
       "      <td id=\"T_d650b_row8_col2\" class=\"data row8 col2\" >0.9909</td>\n",
       "      <td id=\"T_d650b_row8_col3\" class=\"data row8 col3\" >0.9665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d650b_level0_row9\" class=\"row_heading level0 row9\" >Other</th>\n",
       "      <td id=\"T_d650b_row9_col0\" class=\"data row9 col0\" >0.5495</td>\n",
       "      <td id=\"T_d650b_row9_col1\" class=\"data row9 col1\" >0.5994</td>\n",
       "      <td id=\"T_d650b_row9_col2\" class=\"data row9 col2\" >0.4953</td>\n",
       "      <td id=\"T_d650b_row9_col3\" class=\"data row9 col3\" >0.5862</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x70bb87fed940>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recalls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a04e67d-6aa5-4893-9cce-3a35970ae3c3",
   "metadata": {},
   "source": [
    "When looking at the F1 scores, it becomes immediately obvious that the Decision Tree and the XGBoost model are extremely close. The Random Forest has frankly terrible scores in comparison, and the Logistic Regression lags behind the Decision Tree and the XGBoost model. The XGBoost model has the slight advantage in F1 score for the rarest class (DHS), but the Decision Tree outperforms in far more classes. However, the difference between the two is so slight that they are functionally the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "862b59ca-e19e-4df1-9b0b-4992195a6d1d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_a5d8e th {\n",
       "  border-bottom: 2px solid white;\n",
       "  text-align: center;\n",
       "  color: white;\n",
       "}\n",
       "#T_a5d8e th.col_heading {\n",
       "  border-bottom: 2px solid white;\n",
       "  text-align: center;\n",
       "  color: white;\n",
       "}\n",
       "#T_a5d8e th.row_heading {\n",
       "  border-right: 2px solid white;\n",
       "  text-align: center;\n",
       "  color: white;\n",
       "}\n",
       "#T_a5d8e caption {\n",
       "  caption-side: top;\n",
       "  text-align: center;\n",
       "}\n",
       "#T_a5d8e_row0_col0, #T_a5d8e_row0_col2, #T_a5d8e_row0_col3, #T_a5d8e_row1_col0, #T_a5d8e_row1_col1, #T_a5d8e_row1_col2, #T_a5d8e_row2_col0, #T_a5d8e_row2_col1, #T_a5d8e_row2_col2, #T_a5d8e_row3_col0, #T_a5d8e_row3_col2, #T_a5d8e_row3_col3, #T_a5d8e_row4_col0, #T_a5d8e_row4_col2, #T_a5d8e_row4_col3, #T_a5d8e_row5_col0, #T_a5d8e_row5_col2, #T_a5d8e_row5_col3, #T_a5d8e_row6_col0, #T_a5d8e_row6_col1, #T_a5d8e_row6_col2, #T_a5d8e_row7_col0, #T_a5d8e_row7_col2, #T_a5d8e_row7_col3, #T_a5d8e_row8_col0, #T_a5d8e_row8_col2, #T_a5d8e_row8_col3, #T_a5d8e_row9_col0, #T_a5d8e_row9_col2, #T_a5d8e_row9_col3 {\n",
       "  text-align: center;\n",
       "  color: white;\n",
       "}\n",
       "#T_a5d8e_row0_col1, #T_a5d8e_row1_col3, #T_a5d8e_row2_col3, #T_a5d8e_row3_col1, #T_a5d8e_row4_col1, #T_a5d8e_row5_col1, #T_a5d8e_row6_col3, #T_a5d8e_row7_col1, #T_a5d8e_row8_col1, #T_a5d8e_row9_col1 {\n",
       "  background-color: #355C7D;\n",
       "  text-align: center;\n",
       "  color: white;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_a5d8e\">\n",
       "  <caption><b style='font-size:16px; color:white'>F1 Score per Class by Model</b></caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_a5d8e_level0_col0\" class=\"col_heading level0 col0\" >SGDClassifier</th>\n",
       "      <th id=\"T_a5d8e_level0_col1\" class=\"col_heading level0 col1\" >DecisionTreeClassifier</th>\n",
       "      <th id=\"T_a5d8e_level0_col2\" class=\"col_heading level0 col2\" >RandomForestClassifier</th>\n",
       "      <th id=\"T_a5d8e_level0_col3\" class=\"col_heading level0 col3\" >XGBClassifier</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Class</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_a5d8e_level0_row0\" class=\"row_heading level0 row0\" >DEP</th>\n",
       "      <td id=\"T_a5d8e_row0_col0\" class=\"data row0 col0\" >0.9013</td>\n",
       "      <td id=\"T_a5d8e_row0_col1\" class=\"data row0 col1\" >0.9077</td>\n",
       "      <td id=\"T_a5d8e_row0_col2\" class=\"data row0 col2\" >0.8943</td>\n",
       "      <td id=\"T_a5d8e_row0_col3\" class=\"data row0 col3\" >0.9070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a5d8e_level0_row1\" class=\"row_heading level0 row1\" >DHS</th>\n",
       "      <td id=\"T_a5d8e_row1_col0\" class=\"data row1 col0\" >0.7069</td>\n",
       "      <td id=\"T_a5d8e_row1_col1\" class=\"data row1 col1\" >0.7304</td>\n",
       "      <td id=\"T_a5d8e_row1_col2\" class=\"data row1 col2\" >0.5819</td>\n",
       "      <td id=\"T_a5d8e_row1_col3\" class=\"data row1 col3\" >0.7325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a5d8e_level0_row2\" class=\"row_heading level0 row2\" >DOB</th>\n",
       "      <td id=\"T_a5d8e_row2_col0\" class=\"data row2 col0\" >0.8563</td>\n",
       "      <td id=\"T_a5d8e_row2_col1\" class=\"data row2 col1\" >0.8562</td>\n",
       "      <td id=\"T_a5d8e_row2_col2\" class=\"data row2 col2\" >0.8519</td>\n",
       "      <td id=\"T_a5d8e_row2_col3\" class=\"data row2 col3\" >0.8585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a5d8e_level0_row3\" class=\"row_heading level0 row3\" >DOHMH</th>\n",
       "      <td id=\"T_a5d8e_row3_col0\" class=\"data row3 col0\" >0.7664</td>\n",
       "      <td id=\"T_a5d8e_row3_col1\" class=\"data row3 col1\" >0.8099</td>\n",
       "      <td id=\"T_a5d8e_row3_col2\" class=\"data row3 col2\" >0.5758</td>\n",
       "      <td id=\"T_a5d8e_row3_col3\" class=\"data row3 col3\" >0.8053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a5d8e_level0_row4\" class=\"row_heading level0 row4\" >DOT</th>\n",
       "      <td id=\"T_a5d8e_row4_col0\" class=\"data row4 col0\" >0.7050</td>\n",
       "      <td id=\"T_a5d8e_row4_col1\" class=\"data row4 col1\" >0.7194</td>\n",
       "      <td id=\"T_a5d8e_row4_col2\" class=\"data row4 col2\" >0.7069</td>\n",
       "      <td id=\"T_a5d8e_row4_col3\" class=\"data row4 col3\" >0.7136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a5d8e_level0_row5\" class=\"row_heading level0 row5\" >DPR</th>\n",
       "      <td id=\"T_a5d8e_row5_col0\" class=\"data row5 col0\" >0.8118</td>\n",
       "      <td id=\"T_a5d8e_row5_col1\" class=\"data row5 col1\" >0.8241</td>\n",
       "      <td id=\"T_a5d8e_row5_col2\" class=\"data row5 col2\" >0.7778</td>\n",
       "      <td id=\"T_a5d8e_row5_col3\" class=\"data row5 col3\" >0.8216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a5d8e_level0_row6\" class=\"row_heading level0 row6\" >DSNY</th>\n",
       "      <td id=\"T_a5d8e_row6_col0\" class=\"data row6 col0\" >0.7507</td>\n",
       "      <td id=\"T_a5d8e_row6_col1\" class=\"data row6 col1\" >0.7649</td>\n",
       "      <td id=\"T_a5d8e_row6_col2\" class=\"data row6 col2\" >0.5262</td>\n",
       "      <td id=\"T_a5d8e_row6_col3\" class=\"data row6 col3\" >0.7652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a5d8e_level0_row7\" class=\"row_heading level0 row7\" >HPD</th>\n",
       "      <td id=\"T_a5d8e_row7_col0\" class=\"data row7 col0\" >0.9817</td>\n",
       "      <td id=\"T_a5d8e_row7_col1\" class=\"data row7 col1\" >0.9856</td>\n",
       "      <td id=\"T_a5d8e_row7_col2\" class=\"data row7 col2\" >0.9763</td>\n",
       "      <td id=\"T_a5d8e_row7_col3\" class=\"data row7 col3\" >0.9852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a5d8e_level0_row8\" class=\"row_heading level0 row8\" >NYPD</th>\n",
       "      <td id=\"T_a5d8e_row8_col0\" class=\"data row8 col0\" >0.9323</td>\n",
       "      <td id=\"T_a5d8e_row8_col1\" class=\"data row8 col1\" >0.9360</td>\n",
       "      <td id=\"T_a5d8e_row8_col2\" class=\"data row8 col2\" >0.9006</td>\n",
       "      <td id=\"T_a5d8e_row8_col3\" class=\"data row8 col3\" >0.9352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a5d8e_level0_row9\" class=\"row_heading level0 row9\" >Other</th>\n",
       "      <td id=\"T_a5d8e_row9_col0\" class=\"data row9 col0\" >0.6693</td>\n",
       "      <td id=\"T_a5d8e_row9_col1\" class=\"data row9 col1\" >0.7136</td>\n",
       "      <td id=\"T_a5d8e_row9_col2\" class=\"data row9 col2\" >0.6395</td>\n",
       "      <td id=\"T_a5d8e_row9_col3\" class=\"data row9 col3\" >0.7088</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x70bb87ff4fb0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcffa4fe-0580-4ab3-8c1a-aed194b768c6",
   "metadata": {},
   "source": [
    "Overall, either the Decision Tree or the XGBoost model would be suitable choices for directing 311 calls to various NY agencies. Their performance is very similar across the board. The Decision Tree is useful in that it is more interpretable. The decision tree itself can be generated and shown to analyze all of the steps that make up its decision making process. The XGBoost model has the benefit of being able to utilize GPUs while training, making it more likely to scale as the size of the data grows, since it can leverage more powerful hardware to gain performance over the normal sci-kit learn models.\n",
    "\n",
    "As the full size of the 311 dataset is very large and only continues to grow over time, the computational efficiency of the XGBoost model is likely more important than the interpretability of the Decision Tree model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
